{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "class LanguageTranslator:\n",
    "    def __init__(self, model_identifier: str = 'Helsinki-NLP/opus-mt-en-fr'):\n",
    "        self.tokenizer = MarianTokenizer.from_pretrained(model_identifier)\n",
    "        self.model = MarianMTModel.from_pretrained(model_identifier)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        return text.lower()\n",
    "\n",
    "    def translate_text_to_french(self, text: str) -> str:\n",
    "        preprocessed_text = self.preprocess_text(text)\n",
    "        encoded_text = self.tokenizer(preprocessed_text, return_tensors=\"pt\")\n",
    "        generated_tokens = self.model.generate(**encoded_text)\n",
    "        translated_text = self.tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        return translated_text\n",
    "\n",
    "translator = LanguageTranslator()\n",
    "input_text = \"The cat sat on the mat.\"\n",
    "output_text = translator.translate_text_to_french(input_text)\n",
    "print(f\"Translated text: {output_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ANN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Create a Model Class that inherits nn.Module\n",
    "class Model(nn.Module):\n",
    " \n",
    "  # output (3 classes of iris flowers)\n",
    "  def __init__(self, in_features=4, h1=8, h2=9, out_features=3):\n",
    "    super().__init__() # instantiate our nn.Module\n",
    "    self.fc1 = nn.Linear(in_features, h1)\n",
    "    self.fc2 = nn.Linear(h1, h2)\n",
    "    self.out = nn.Linear(h2, out_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.out(x)\n",
    "\n",
    "    return x\n",
    "# Pick a manual seed for randomization\n",
    "torch.manual_seed(41)\n",
    "# Create an instance of model\n",
    "model = Model()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "url = 'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv'\n",
    "my_df = pd.read_csv(url)\n",
    "my_df\n",
    "# Change last column from strings to integers\n",
    "my_df['variety'] = my_df['variety'].replace('Setosa', 0.0)\n",
    "my_df['variety'] = my_df['variety'].replace('Versicolor', 1.0)\n",
    "my_df['variety'] = my_df['variety'].replace('Virginica', 2.0)\n",
    "my_df\n",
    "# Train Test Split/ Set X, y\n",
    "X = my_df.drop('variety', axis=1)\n",
    "y = my_df['variety']\n",
    "X_np = my_df.drop('variety', axis=1).to_numpy()\n",
    "y_np = my_df['variety'].to_numpy()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "epochs = 100\n",
    "losses = []\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    y_pred = model(X_train_tensor)\n",
    "    \n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f'Epoch {i+1}, Loss: {loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "  y_pred = model(X_train_tensor)\n",
    "  train_loss = criterion(y_pred, y_train_tensor)\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  train_loss.backward()\n",
    "  optimizer.step()\n",
    "  train_losses.append(train_loss.item())\n",
    "\n",
    "  val_y_pred = model(X_test_tensor)\n",
    "  val_loss = criterion(val_y_pred, y_test_tensor)\n",
    "  val_losses.append(val_loss.item())\n",
    "  if (i + 1) % 10 == 0:\n",
    "    print(f'Epoch {i+1}, Training Loss: {train_loss.item():.4f}, Validation Loss: {val_loss.item():.4f}')\n",
    "\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "    loss = criterion(y_pred_tensor, y_test_tensor)\n",
    "    print(\"Test Loss:\", loss.item())\n",
    "\n",
    "#classification\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "  for i, data in enumerate(X_test_tensor):\n",
    "    y_val = model.forward(data)\n",
    "\n",
    "    if y_test[i] == 0:\n",
    "      x = \"Setosa\"\n",
    "    elif y_test[i] == 1:\n",
    "      x = 'Versicolor'\n",
    "    else:\n",
    "      x = 'Virginica'\n",
    "\n",
    "\n",
    "    # Will tell us what type of flower class our network thinks it is\n",
    "    print(f'{i+1}.)  {str(y_val)} \\t {y_test[i]} \\t {y_val.argmax().item()}')\n",
    "\n",
    "    # Correct or not\n",
    "    if y_val.argmax().item() == y_test[i]:\n",
    "      correct +=1\n",
    "\n",
    "print(f'We got {correct} correct!')\n",
    "\n",
    "new_iris = torch.tensor([4.7, 3.2, 1.3, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data = pd.read_csv(\"diabetes.csv\")\n",
    "data.head()\n",
    "zero_not_accepted = ['Glucose', 'BloodPressure',\n",
    "                     'SkinThickness', 'BMI', 'Insulin']\n",
    "for col in zero_not_accepted:\n",
    "    data[col] = data[col].replace(0, np.NaN)\n",
    "    mean = int(data[col].mean(skipna=True))\n",
    "    data[col] = data[col].replace(np.NaN, mean)\n",
    "x = data.iloc[:,0:8]\n",
    "y = data.iloc[:,8]\n",
    "sns.heatmap(data.corr())\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 0)\n",
    "def euclidean_distance(instance1, instance2):\n",
    "    return np.sqrt(np.sum((instance1 - instance2) ** 2))\n",
    "\n",
    "def knn_classifier(K, train_data, train_labels, test_data):\n",
    "    predicted_labels = []\n",
    "    for test_point in test_data:\n",
    "        distances = []\n",
    "        for train_point, label in zip(train_data, train_labels):\n",
    "            dist = euclidean_distance(test_point, train_point)\n",
    "            distances.append((dist, label))\n",
    "        distances = sorted(distances)[:K]\n",
    "        labels = [label for dist, label in distances]\n",
    "        predicted_labels.append(max(set(labels), key=labels.count))\n",
    "    return predicted_labels\n",
    "\n",
    "def evaluate_knn(K_values, train_data, test_data, train_labels, test_labels):\n",
    "    for k in K_values:\n",
    "        predicted_labels = knn_classifier(k, train_data, train_labels, test_data)\n",
    "        accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "        print(f\"Value of K = {k}, Accuracy = {accuracy}\")\n",
    "        print(\"Actual Values:\", test_labels)\n",
    "        print(\"Predicted Values:\", predicted_labels)\n",
    "        print()\n",
    "K_values = [2, 4, 5, 7]\n",
    "evaluate_knn(K_values, x_train.values, x_test.values, y_train, y_test)\n",
    "\n",
    "# Plotting example (you can add more visualizations as needed)\n",
    "if x_test.shape[1] >= 2:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=x_test.iloc[:, 0], y=x_test.iloc[:, 1], hue=y_test, palette=\"Set2\")\n",
    "    plt.title(\"Scatter plot of two features with actual labels\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.legend(title=\"Label\", loc=\"best\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient features for plotting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genetic Algorithm\n",
    "import random\n",
    "# Define the problem parameters\n",
    "secret_word = 'agenda'\n",
    "word_length = len(secret_word)\n",
    "population_size = 50\n",
    "generations = 100\n",
    "# Create the initial population\n",
    "population = []\n",
    "for i in range(population_size):\n",
    "    individual = ''.join(random.choice('abcdefghijklmnopqrstuvwxyz') for j in range(word_length))\n",
    "    population.append(individual)\n",
    "# Define the fitness function\n",
    "def fitness(individual):\n",
    "    score = 0\n",
    "    for i in range(word_length):\n",
    "        if individual[i] == secret_word[i]:\n",
    "            score += 1\n",
    "    return score\n",
    "# Define the selection function\n",
    "def selection(population):\n",
    "    total_fitness = sum(fitness(individual) for individual in population)\n",
    "\n",
    "    probability_wheel = []\n",
    "    for individual in population:\n",
    "        probability = fitness(individual) / total_fitness\n",
    "        probability_wheel.append(probability)\n",
    "\n",
    "    parents = []\n",
    "    for _ in range(2):\n",
    "        random_value = random.random()\n",
    "        current_sum = 0\n",
    "        for i, probability in enumerate(probability_wheel):\n",
    "            current_sum += probability\n",
    "            if current_sum >= random_value:\n",
    "                parents.append(population[i])\n",
    "                break\n",
    "\n",
    "    return parents\n",
    "def crossover(parents):\n",
    "    parent1, parent2 = parents\n",
    "\n",
    "    crossover_points = sorted(random.sample(range(1, len(parent1) - 1), 2))\n",
    "\n",
    "    child1 = parent1[:crossover_points[0]] + parent2[crossover_points[0]:crossover_points[1]] + parent1[crossover_points[1]:]\n",
    "    child2 = parent2[:crossover_points[0]] + parent1[crossover_points[0]:crossover_points[1]] + parent2[crossover_points[1]:]\n",
    "\n",
    "    return child1, child2\n",
    "# Define the mutation function\n",
    "def mutation(individual, probability):\n",
    "    individual_list = list(individual)\n",
    "    for i in range(word_length):\n",
    "        if random.random() < probability:\n",
    "            individual_list[i] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "\n",
    "    return ''.join(individual_list)\n",
    "# Run the genetic algorithm\n",
    "for generation in range(generations):\n",
    "    print(f\"Generation {generation + 1}\")\n",
    "    # Selection\n",
    "    new_population = []\n",
    "    for i in range(population_size // 2):\n",
    "        parents = selection(population)\n",
    "        # Crossover\n",
    "        child1, child2 = crossover(parents)\n",
    "        # Mutation\n",
    "        child1 = mutation(child1, 0.1)\n",
    "        child2 = mutation(child2, 0.1)\n",
    "        new_population.append(child1)\n",
    "        new_population.append(child2)\n",
    "\n",
    "    # Keep the best individual from the previous generation\n",
    "    new_population.insert(0, max(population, key=fitness))\n",
    "    population = new_population\n",
    "\n",
    "    # Print the best individual\n",
    "    best_individual = max(population, key=fitness)\n",
    "    print(f\"Best individual: {best_individual}\")\n",
    "    if best_individual == secret_word:\n",
    "        print(\"Solution found!\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSP\n",
    "def assign_packages(packages, cities, city_capacity, allowed_cities):\n",
    "\n",
    "    def is_consistent(package, city):\n",
    "\n",
    "        if city_capacity.get(city, 0) <= 0:        #Capacity Chacking\n",
    "\n",
    "            return False\n",
    "        \n",
    "        \n",
    "        if package in allowed_cities and city not in allowed_cities[package]:\n",
    "        \n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def backtrack_assignment(assignment):\n",
    "        \n",
    "        \n",
    "        if len(assignment) == len(packages):\n",
    "\n",
    "            return assignment\n",
    "        \n",
    "        \n",
    "        package = None\n",
    "\n",
    "        for p in packages:\n",
    "        \n",
    "            if p not in assignment:\n",
    "        \n",
    "                package = p\n",
    "        \n",
    "                break\n",
    "        \n",
    "        \n",
    "        for city in cities:\n",
    "\n",
    "            if is_consistent(package, city):\n",
    "\n",
    "                \n",
    "                assignment[package] = city\n",
    "\n",
    "                city_capacity[city] -= 1\n",
    "                \n",
    "                \n",
    "                result = backtrack_assignment(assignment)\n",
    "                \n",
    "               \n",
    "                if result is not None:\n",
    "\n",
    "                    return result\n",
    "                \n",
    "                \n",
    "                assignment.pop(package)\n",
    "\n",
    "                city_capacity[city] += 1\n",
    "        \n",
    "    \n",
    "        return None\n",
    "\n",
    "    \n",
    "    assignment = {}\n",
    "\n",
    "    optimal_solution = backtrack_assignment(assignment)  #Backtracking to check optimal solution\n",
    "    \n",
    "    \n",
    "    solutions = [(package, optimal_solution[package]) for package in packages]\n",
    "    \n",
    "    return solutions\n",
    "\n",
    "\n",
    "packages = [1, 2, 3, 4]\n",
    "cities = ['A', 'B', 'C']\n",
    "city_capacity = {'A': 2, 'B': 3, 'C': 1}\n",
    "allowed_cities = {1: ['A', 'B'], 2: ['B', 'C'], 3: ['A', 'C'], 4: ['A']}\n",
    "\n",
    "\n",
    "solutions = assign_packages(packages, cities, city_capacity.copy(), allowed_cities)\n",
    "\n",
    "\n",
    "for package, city in solutions:\n",
    "    print(f\"Package {package} is assigned to City {city}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A piori\n",
    "import heapq\n",
    "\n",
    "class MapGraph:\n",
    "    def __init__(self, adjac_lis):\n",
    "        self.adjac_lis = adjac_lis\n",
    " \n",
    "    def get_neighbors(self, v):\n",
    "        return self.adjac_lis[v]\n",
    " \n",
    "    # This is a heuristic function which has equal values for all nodes\n",
    "    def heuristic(self, n):\n",
    "        H = {\n",
    "            'Arad': 366,\n",
    "            'Oradea': 380,\n",
    "            'Timisoara': 329,\n",
    "            'Zerind': 374,\n",
    "            'Lugoj': 244,\n",
    "            'Mehadia': 241, \n",
    "            'Drobeta': 242,\n",
    "            'Craiova': 160,\n",
    "            'Sibiu': 253,\n",
    "            'Fagaras': 176,\n",
    "            'Rimnicu Vilcea': 193,\n",
    "            'Pitesti': 100,\n",
    "            'Bucharest': 0\n",
    "        }\n",
    "        return H[n]\n",
    "\n",
    "    def a_star_algorithm(self, start, goal):\n",
    "        open_set = set([start])\n",
    "        came_from = {}\n",
    "        cost_so_far = {node: float('infinity') for node in self.adjac_lis}\n",
    "        cost_so_far[start] = 0\n",
    "        estimated_total_cost = {node: float('infinity') for node in self.adjac_lis}\n",
    "        estimated_total_cost[start] = self.heuristic(start)\n",
    "\n",
    "        while open_set:\n",
    "            current = None\n",
    "            current_estimated_cost = None\n",
    "            for node in open_set:\n",
    "                if current is None or estimated_total_cost[node] < current_estimated_cost:\n",
    "                    current = node\n",
    "                    current_estimated_cost = estimated_total_cost[node]\n",
    "\n",
    "            if current == goal:\n",
    "                path = []\n",
    "                while current in came_from:\n",
    "                    path.append(current)\n",
    "                    current = came_from[current]\n",
    "                path.append(start)\n",
    "                path.reverse()\n",
    "                print(\"Optimal Path:\", path)\n",
    "                print(\"Path Cost:\", cost_so_far[goal])\n",
    "                return path, cost_so_far[goal]\n",
    "\n",
    "            open_set.remove(current)\n",
    "            for neighbor, cost in self.get_neighbors(current):\n",
    "                new_cost = cost_so_far[current] + cost\n",
    "                if new_cost < cost_so_far[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    cost_so_far[neighbor] = new_cost\n",
    "                    estimated_total_cost[neighbor] = new_cost + self.heuristic(neighbor)\n",
    "                    if neighbor not in open_set:\n",
    "                        open_set.add(neighbor)\n",
    "\n",
    "        print(\"No path found\")\n",
    "        return None, None\n",
    "\n",
    "adjacency_list = {\n",
    "    'Arad': [('Timisoara', 118), ('Zerind', 75), ('Sibiu', 140)],\n",
    "    'Timisoara': [('Arad', 118), ('Lugoj', 111)],\n",
    "    'Oradea': [('Sibiu', 151), ('Zerind', 71)],\n",
    "    'Zerind': [('Oradea', 71), ('Arad', 75)],\n",
    "    'Lugoj': [('Timisoara', 111), ('Mehadia', 70)],\n",
    "    'Mehadia': [('Lugoj', 70), ('Drobeta', 75)],\n",
    "    'Drobeta': [('Mehadia', 75), ('Craiova', 120)],\n",
    "    'Craiova': [('Drobeta', 120), ('Rimnicu Vilcea', 146), ('Pitesti', 138)],\n",
    "    'Sibiu': [('Arad', 140), ('Oradea', 151), ('Fagaras', 99), ('Rimnicu Vilcea', 80)],\n",
    "    'Rimnicu Vilcea': [('Sibiu', 80), ('Pitesti', 97), ('Craiova', 146)],\n",
    "    'Fagaras': [('Sibiu', 99), ('Bucharest', 211)],\n",
    "    'Pitesti': [('Bucharest', 101), ('Craiova', 138), ('Rimnicu Vilcea', 97)],\n",
    "    'Bucharest': [('Pitesti', 101), ('Fagaras', 211)],\n",
    "}\n",
    "\n",
    "graph = MapGraph(adjacency_list)\n",
    "graph.a_star_algorithm('Arad', 'Bucharest')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
